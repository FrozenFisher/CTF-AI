"""
ç›´æ¥è®­ç»ƒè„šæœ¬ - ä¸ä¾èµ–æ¸¸æˆæœåŠ¡å™¨
ç›´æ¥æ¨¡æ‹Ÿæ¸¸æˆç¯å¢ƒè¿›è¡Œè®­ç»ƒï¼Œé€Ÿåº¦æ›´å¿«
"""

import sys
import os
# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿è®¿é—® lib/ å’Œ pathfinding_adapter.py
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(SCRIPT_DIR)
if PARENT_DIR not in sys.path:
    sys.path.insert(0, PARENT_DIR)

from lib import RL
import numpy as np
import random
import math
import json
import time
import torch
from collections import deque
from multiprocessing import Process, Queue, Manager, Value, Lock
import multiprocessing

# å¯¼å…¥è·¯å¾„è§„åˆ’é€‚é…å™¨ï¼ˆå¿…éœ€ï¼‰
try:
    import pathfinding_adapter as pf
except ImportError as e:
    print("=" * 60)
    print("é”™è¯¯: è·¯å¾„è§„åˆ’é€‚é…å™¨ä¸å¯ç”¨ï¼")
    print("=" * 60)
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("\nè¯·ç¡®ä¿ pathfinding_adapter.py æ–‡ä»¶å­˜åœ¨ä¸”å¯å¯¼å…¥")
    print("è·¯å¾„è§„åˆ’æ˜¯å¿…éœ€çš„ï¼Œæ— æ³•ä½¿ç”¨ç®€åŒ–ç§»åŠ¨ä½œä¸ºå›é€€")
    import sys
    sys.exit(1)

# ç®€åŒ–çš„æ¸¸æˆæ¨¡æ‹Ÿå™¨
class SimpleGameSimulator:
    """ç®€åŒ–çš„æ¸¸æˆæ¨¡æ‹Ÿå™¨ï¼Œç”¨äºç›´æ¥è®­ç»ƒ"""
    
    def __init__(self, width=20, height=20, num_players=3, num_flags=9, num_obstacles_1=8, num_obstacles_2=4):
        self.width = width
        self.height = height
        self.num_players = num_players
        self.num_flags = num_flags
        self.num_obstacles_1 = num_obstacles_1  # å•æ ¼éšœç¢æ•°é‡
        self.num_obstacles_2 = num_obstacles_2  # åŒæ ¼éšœç¢æ•°é‡
        self.middle_line = width / 2
        
        # åˆå§‹åŒ–æ¸¸æˆçŠ¶æ€
        self.reset()
    
    def _not_contains(self, xy_list, x, y):
        """æ£€æŸ¥ä½ç½®æ˜¯å¦ä¸åœ¨åˆ—è¡¨ä¸­"""
        return not any(pos[0] == x and pos[1] == y for pos in xy_list)
    
    def _is_valid_position(self, x, y, obstacles1, obstacles2, exclude_list=None):
        """æ£€æŸ¥ä½ç½®æ˜¯å¦æœ‰æ•ˆï¼ˆä¸åœ¨éšœç¢ç‰©ä¸Šï¼Œä¸åœ¨æ’é™¤åˆ—è¡¨ä¸­ï¼‰"""
        # æ£€æŸ¥è¾¹ç•Œ
        if x < 1 or x >= self.width - 1 or y < 1 or y >= self.height - 1:
            return False
        
        # æ£€æŸ¥å•æ ¼éšœç¢
        if not self._not_contains(obstacles1, x, y):
            return False
        
        # æ£€æŸ¥åŒæ ¼éšœç¢ï¼ˆå ç”¨yå’Œy+1ï¼‰
        # å¦‚æœ(x, y)æˆ–(x, y-1)åœ¨obstacles2ä¸­ï¼Œåˆ™ä½ç½®æ— æ•ˆ
        if (x, y) in obstacles2 or (x, y - 1) in obstacles2:
            return False
        
        # æ£€æŸ¥æ’é™¤åˆ—è¡¨
        if exclude_list:
            if not self._not_contains(exclude_list, x, y):
                return False
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å›ºå®šçš„targetæˆ–prisonåŒºåŸŸï¼ˆè¿™äº›åŒºåŸŸä¸èƒ½æ”¾éšœç¢å’Œflagï¼‰
        center_y = self.height // 2
        # Lé˜ŸtargetåŒºåŸŸ
        if 2 <= x < 5 and center_y - 1 <= y < center_y + 2:
            return False
        # Ré˜ŸtargetåŒºåŸŸ
        if self.width - 5 <= x < self.width - 2 and center_y - 1 <= y < center_y + 2:
            return False
        # Lé˜ŸprisonåŒºåŸŸ
        if 2 <= x < 5 and self.height - 4 <= y < self.height - 1:
            return False
        # Ré˜ŸprisonåŒºåŸŸ
        if self.width - 5 <= x < self.width - 2 and self.height - 4 <= y < self.height - 1:
            return False
        
        return True
    
    def reset(self):
        """é‡ç½®æ¸¸æˆçŠ¶æ€ï¼Œéšæœºç”Ÿæˆåœ°å›¾ã€flagå’Œå‡ºç”Ÿç‚¹"""
        import random
        
        # ç”Ÿæˆéšæœºéšœç¢ç‰©
        self.obstacles1 = []  # å•æ ¼éšœç¢
        for i in range(self.num_obstacles_1):
            while True:
                x = random.randint(4, self.width - 5)
                y = random.randint(1, self.height - 2)
                if self._is_valid_position(x, y, self.obstacles1, []):
                    self.obstacles1.append((x, y))
                    break
        
        self.obstacles2 = []  # åŒæ ¼éšœç¢ï¼ˆå ç”¨yå’Œy+1ï¼‰
        for i in range(self.num_obstacles_2):
            attempts = 0
            while attempts < 1000:  # é˜²æ­¢æ— é™å¾ªç¯
                x = random.randint(4, self.width - 5)
                y = random.randint(1, self.height - 3)  # y+1æœ€å¤§ä¸ºheight-2ï¼Œåœ¨æœ‰æ•ˆèŒƒå›´å†…
                # æ£€æŸ¥yå’Œy+1ä½ç½®éƒ½æœ‰æ•ˆï¼Œä¸”y+1ä¸åœ¨target/prisonåŒºåŸŸ
                if (self._is_valid_position(x, y, self.obstacles1, self.obstacles2, None) and
                    self._is_valid_position(x, y + 1, self.obstacles1, self.obstacles2, None)):
                    self.obstacles2.append((x, y))
                    break
                attempts += 1
            if attempts >= 1000:
                print(f"è­¦å‘Š: æ— æ³•ç”Ÿæˆç¬¬{i+1}ä¸ªåŒæ ¼éšœç¢ï¼Œè·³è¿‡")
        
        # ç›®æ ‡åŒºåŸŸï¼ˆå›ºå®šä½ç½®ï¼‰
        self.l_targets = set()
        center_y = self.height // 2
        for x in range(2, 5):
            for y in range(center_y - 1, center_y + 2):
                self.l_targets.add((x, y))
        
        self.r_targets = set()
        for x in range(self.width - 5, self.width - 2):
            for y in range(center_y - 1, center_y + 2):
                self.r_targets.add((x, y))
        
        # PrisonåŒºåŸŸï¼ˆå›ºå®šä½ç½®ï¼‰
        self.l_prison = set()
        for x in range(2, 5):
            for y in range(self.height - 4, self.height - 1):
                self.l_prison.add((x, y))
        
        self.r_prison = set()
        for x in range(self.width - 5, self.width - 2):
            for y in range(self.height - 4, self.height - 1):
                self.r_prison.add((x, y))
        
        # éšæœºç”ŸæˆLé˜Ÿflagï¼ˆåœ¨å·¦ä¾§åŠåœºï¼‰
        self.l_flags = []
        l_flag_positions = []
        for i in range(self.num_flags):
            while True:
                x = random.randint(2, self.width // 2 - 1)
                y = random.randint(1, self.height - 3)
                if self._is_valid_position(x, y, self.obstacles1, self.obstacles2, l_flag_positions):
                    l_flag_positions.append((x, y))
                    self.l_flags.append({
                        "posX": x,
                        "posY": y,
                        "canPickup": True,
                        "mine": True
                    })
                    break
        
        # éšæœºç”ŸæˆRé˜Ÿflagï¼ˆåœ¨å³ä¾§åŠåœºï¼‰
        self.r_flags = []
        r_flag_positions = []
        for i in range(self.num_flags):
            while True:
                x = random.randint(self.width // 2, self.width - 2)
                y = random.randint(1, self.height - 3)
                if self._is_valid_position(x, y, self.obstacles1, self.obstacles2, r_flag_positions):
                    r_flag_positions.append((x, y))
                    self.r_flags.append({
                        "posX": x,
                        "posY": y,
                        "canPickup": True,
                        "mine": False
                    })
                    break
        
        # éšæœºç”ŸæˆLé˜Ÿç©å®¶å‡ºç”Ÿç‚¹ï¼ˆåœ¨å·¦ä¾§åŠåœºï¼Œé¿å¼€éšœç¢ï¼‰
        self.l_players = []
        l_spawn_positions = []
        for i in range(self.num_players):
            while True:
                x = random.randint(1, self.width // 2 - 1)
                y = random.randint(1, self.height - 2)
                if (self._is_valid_position(x, y, self.obstacles1, self.obstacles2, l_spawn_positions) and
                    (x, y) not in self.l_targets and (x, y) not in self.l_prison):
                    l_spawn_positions.append((x, y))
                    self.l_players.append({
                        "name": f"L{i}",
                        "posX": x,
                        "posY": y,
                        "hasFlag": False,
                        "inPrison": False,
                        "team": "L"
                    })
                    break
        
        # éšæœºç”ŸæˆRé˜Ÿç©å®¶å‡ºç”Ÿç‚¹ï¼ˆåœ¨å³ä¾§åŠåœºï¼Œé¿å¼€éšœç¢ï¼‰
        self.r_players = []
        r_spawn_positions = []
        for i in range(self.num_players):
            while True:
                x = random.randint(self.width // 2, self.width - 2)
                y = random.randint(1, self.height - 2)
                if (self._is_valid_position(x, y, self.obstacles1, self.obstacles2, r_spawn_positions) and
                    (x, y) not in self.r_targets and (x, y) not in self.r_prison):
                    r_spawn_positions.append((x, y))
                    self.r_players.append({
                        "name": f"R{i}",
                        "posX": x,
                        "posY": y,
                        "hasFlag": False,
                        "inPrison": False,
                        "team": "R"
                    })
                    break
        
        # å¾—åˆ†
        self.l_score = 0
        self.r_score = 0
        
        # æ¸¸æˆæ—¶é—´
        self.time = 0
        self.max_time = 300  # æœ€å¤§æ—¶é—´æ­¥æ•°
    
    def is_on_left(self, pos):
        """åˆ¤æ–­ä½ç½®æ˜¯å¦åœ¨å·¦ä¾§"""
        return pos[0] < self.middle_line
    
    def list_players(self, mine, inPrison, hasFlag):
        """åˆ—å‡ºç©å®¶"""
        players = self.l_players if mine else self.r_players
        result = []
        for p in players:
            if (inPrison is None or p["inPrison"] == inPrison) and \
               (hasFlag is None or p["hasFlag"] == hasFlag):
                result.append(p)
        return result
    
    def list_flags(self, mine, canPickup):
        """åˆ—å‡ºflag"""
        flags = self.l_flags if mine else self.r_flags
        result = []
        for f in flags:
            if canPickup is None or f.get("canPickup", True) == canPickup:
                result.append(f)
        return result
    
    def list_targets(self, mine):
        """åˆ—å‡ºç›®æ ‡åŒºåŸŸ"""
        return self.l_targets if mine else self.r_targets
    
    def list_prisons(self, mine):
        """åˆ—å‡ºprisonåŒºåŸŸ"""
        return self.l_prison if mine else self.r_prison
    
    def _is_obstacle(self, x, y):
        """æ£€æŸ¥ä½ç½®æ˜¯å¦æ˜¯éšœç¢ç‰©"""
        # æ£€æŸ¥å•æ ¼éšœç¢
        if (x, y) in self.obstacles1:
            return True
        # æ£€æŸ¥åŒæ ¼éšœç¢ï¼ˆå ç”¨yå’Œy+1ï¼‰
        if (x, y) in self.obstacles2 or (x, y - 1) in self.obstacles2:
            return True
        return False
    
    def apply_action(self, player_name, direction):
        """åº”ç”¨åŠ¨ä½œ"""
        # æ‰¾åˆ°ç©å®¶
        player = None
        for p in self.l_players + self.r_players:
            if p["name"] == player_name:
                player = p
                break
        
        if not player or player["inPrison"]:
            return
        
        # è®¡ç®—æ–°ä½ç½®
        dx, dy = 0, 0
        if direction == "up":
            dy = -1
        elif direction == "down":
            dy = 1
        elif direction == "left":
            dx = -1
        elif direction == "right":
            dx = 1
        
        new_x = player["posX"] + dx
        new_y = player["posY"] + dy
        
        # è¾¹ç•Œæ£€æŸ¥
        if 0 <= new_x < self.width and 0 <= new_y < self.height:
            # æ£€æŸ¥éšœç¢ç‰©
            if not self._is_obstacle(new_x, new_y):
                player["posX"] = new_x
                player["posY"] = new_y
        
        # æ£€æŸ¥æ‹¾å–flag
        if not player["hasFlag"]:
            enemy_flags = self.r_flags if player["team"] == "L" else self.l_flags
            for flag in enemy_flags:
                if flag["canPickup"] and (player["posX"], player["posY"]) == (flag["posX"], flag["posY"]):
                    player["hasFlag"] = True
                    flag["canPickup"] = False
                    break
        
        # æ£€æŸ¥é€è¾¾flag
        if player["hasFlag"]:
            targets = self.l_targets if player["team"] == "L" else self.r_targets
            if (player["posX"], player["posY"]) in targets:
                if player["team"] == "L":
                    self.l_score += 1
                else:
                    self.r_score += 1
                player["hasFlag"] = False
                # é‡ç½®flag
                enemy_flags = self.r_flags if player["team"] == "L" else self.l_flags
                for flag in enemy_flags:
                    if not flag["canPickup"]:
                        flag["canPickup"] = True
        
        # æ£€æŸ¥ç¢°æ’ï¼šæ ¹æ®ç¢°æ’ä½ç½®å†³å®šè°è¿›ç›‘ç‹±
        # åœ¨å·±æ–¹åŠåœºï¼šæŠŠå¯¹æ–¹é€å…¥ç›‘ç‹±
        # åœ¨å¯¹æ–¹åŠåœºï¼šè‡ªå·±è¢«é€å…¥ç›‘ç‹±
        collision_pos = (player["posX"], player["posY"])
        is_collision_on_left = self.is_on_left(collision_pos)
        
        for other in (self.r_players if player["team"] == "L" else self.l_players):
            if other["name"] != player_name and not other["inPrison"]:
                if (other["posX"], other["posY"]) == collision_pos:
                    # å‘ç”Ÿç¢°æ’
                    if player["team"] == "L":
                        # Lé˜Ÿï¼šå·¦è¾¹æ˜¯å·±æ–¹åŠåœºï¼Œå³è¾¹æ˜¯å¯¹æ–¹åŠåœº
                        if is_collision_on_left:
                            # åœ¨å·±æ–¹åŠåœºï¼šæŠŠå¯¹æ–¹ï¼ˆRé˜Ÿï¼‰é€å…¥ç›‘ç‹±
                            other["inPrison"] = True
                            if other["hasFlag"]:
                                other["hasFlag"] = False
                                # é‡ç½®flag
                                for flag in self.r_flags:
                                    if not flag["canPickup"]:
                                        flag["canPickup"] = True
                        else:
                            # åœ¨å¯¹æ–¹åŠåœºï¼šè‡ªå·±ï¼ˆLé˜Ÿï¼‰è¢«é€å…¥ç›‘ç‹±
                            player["inPrison"] = True
                            if player["hasFlag"]:
                                player["hasFlag"] = False
                                # é‡ç½®flag
                                for flag in self.r_flags:
                                    if not flag["canPickup"]:
                                        flag["canPickup"] = True
                    else:  # player["team"] == "R"
                        # Ré˜Ÿï¼šå³è¾¹æ˜¯å·±æ–¹åŠåœºï¼Œå·¦è¾¹æ˜¯å¯¹æ–¹åŠåœº
                        if not is_collision_on_left:
                            # åœ¨å·±æ–¹åŠåœºï¼šæŠŠå¯¹æ–¹ï¼ˆLé˜Ÿï¼‰é€å…¥ç›‘ç‹±
                            other["inPrison"] = True
                            if other["hasFlag"]:
                                other["hasFlag"] = False
                                # é‡ç½®flag
                                for flag in self.l_flags:
                                    if not flag["canPickup"]:
                                        flag["canPickup"] = True
                        else:
                            # åœ¨å¯¹æ–¹åŠåœºï¼šè‡ªå·±ï¼ˆRé˜Ÿï¼‰è¢«é€å…¥ç›‘ç‹±
                            player["inPrison"] = True
                            if player["hasFlag"]:
                                player["hasFlag"] = False
                                # é‡ç½®flag
                                for flag in self.l_flags:
                                    if not flag["canPickup"]:
                                        flag["canPickup"] = True
                    break  # åªå¤„ç†ç¬¬ä¸€ä¸ªç¢°æ’çš„å¯¹æ‰‹
    
    def step(self):
        """æ¸¸æˆæ­¥è¿›"""
        self.time += 1
        return self.time >= self.max_time or self.l_score >= 3 or self.r_score >= 3
    
    def get_state_dict(self):
        """è·å–çŠ¶æ€å­—å…¸ï¼ˆç”¨äºworldå¯¹è±¡ï¼‰"""
        return {
            'width': self.width,
            'height': self.height,
            'l_players': self.l_players,
            'r_players': self.r_players,
            'l_flags': self.l_flags,
            'r_flags': self.r_flags,
            'l_targets': self.l_targets,
            'r_targets': self.r_targets,
            'l_prison': self.l_prison,
            'r_prison': self.r_prison
        }


# åˆ›å»ºç®€åŒ–çš„worldå¯¹è±¡åŒ…è£…å™¨
class SimpleWorldWrapper:
    """å°†SimpleGameSimulatoråŒ…è£…æˆç±»ä¼¼GameMapçš„å¯¹è±¡ï¼Œæ”¯æŒçœŸå®è·¯å¾„è§„åˆ’"""
    
    def __init__(self, simulator, team="L"):
        """
        Args:
            simulator: SimpleGameSimulatorå®ä¾‹
            team: å½“å‰è§†è§’çš„é˜Ÿä¼ ("L" æˆ– "R")
        """
        self.simulator = simulator
        self.width = simulator.width
        self.height = simulator.height
        self.middle_line = simulator.middle_line
        self.team = team  # å½“å‰è§†è§’çš„é˜Ÿä¼
        
        # è·¯å¾„è§„åˆ’æ‰€éœ€çš„å±æ€§
        self.walls = set()  # å¢™å£ï¼ˆå½“å‰ä¸ºç©ºï¼Œå› ä¸ºSimpleGameSimulatoræ²¡æœ‰å¢™å£ï¼‰
        # éšœç¢ç‰©ï¼ˆä»simulatorè·å–ï¼‰
        self.obstacles = set()
        if hasattr(simulator, 'obstacles1'):
            self.obstacles.update(simulator.obstacles1)
        if hasattr(simulator, 'obstacles2'):
            self.obstacles.update(simulator.obstacles2)
            # obstacles2å ç”¨ä¸¤ä¸ªæ ¼å­
            for x, y in simulator.obstacles2:
                self.obstacles.add((x, y + 1))
        
        # ç›®æ ‡åŒºåŸŸï¼ˆä»simulatorè·å–ï¼‰
        if team == "L":
            self.my_team_target = simulator.l_targets
            self.opponent_team_target = simulator.r_targets
        else:
            self.my_team_target = simulator.r_targets
            self.opponent_team_target = simulator.l_targets
    
    def list_players(self, mine, inPrison, hasFlag):
        """æ ¹æ®å½“å‰è§†è§’åˆ—å‡ºç©å®¶"""
        if self.team == "L":
            return self.simulator.list_players(mine, inPrison, hasFlag)
        else:  # Ré˜Ÿè§†è§’ï¼šmineçš„å«ä¹‰ç›¸å
            return self.simulator.list_players(not mine if mine is not None else None, inPrison, hasFlag)
    
    def list_flags(self, mine, canPickup):
        """æ ¹æ®å½“å‰è§†è§’åˆ—å‡ºflag"""
        if self.team == "L":
            return self.simulator.list_flags(mine, canPickup)
        else:  # Ré˜Ÿè§†è§’ï¼šmineçš„å«ä¹‰ç›¸å
            return self.simulator.list_flags(not mine if mine is not None else None, canPickup)
    
    def list_targets(self, mine):
        """æ ¹æ®å½“å‰è§†è§’åˆ—å‡ºç›®æ ‡"""
        if self.team == "L":
            return self.simulator.list_targets(mine)
        else:  # Ré˜Ÿè§†è§’ï¼šmineçš„å«ä¹‰ç›¸å
            return self.simulator.list_targets(not mine if mine is not None else None)
    
    def list_prisons(self, mine):
        """æ ¹æ®å½“å‰è§†è§’åˆ—å‡ºprison"""
        if self.team == "L":
            return self.simulator.list_prisons(mine)
        else:  # Ré˜Ÿè§†è§’ï¼šmineçš„å«ä¹‰ç›¸å
            return self.simulator.list_prisons(not mine if mine is not None else None)
    
    def is_on_left(self, pos):
        if isinstance(pos, tuple):
            return self.simulator.is_on_left(pos)
        elif isinstance(pos, (list, set)) and len(pos) > 0:
            # å¦‚æœæ˜¯é›†åˆï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
            first_pos = next(iter(pos))
            return self.simulator.is_on_left(first_pos)
        return pos[0] < self.middle_line if isinstance(pos, (tuple, list)) else False
    
    def route_to(self, srcXY, dstXY, extra_obstacles=None):
        """
        BFSè·¯å¾„æœç´¢ï¼ˆä¸game_engine.GameMap.route_toç›¸åŒï¼‰
        """
        import collections
        extras = set(extra_obstacles) if extra_obstacles else set()
        queue = collections.deque([[srcXY]])
        seen = {srcXY}
        
        while queue:
            path = queue.popleft()
            curr = path[-1]
            if curr == dstXY:
                return path

            for dx, dy in [(0, -1), (0, 1), (-1, 0), (1, 0)]:  # Up, Down, Left, Right
                nxt = (curr[0] + dx, curr[1] + dy)
                if (0 <= nxt[0] < self.width and 0 <= nxt[1] < self.height and 
                    nxt not in self.walls and nxt not in self.obstacles and 
                    nxt not in extras and nxt not in seen):
                    queue.append(path + [nxt])
                    seen.add(nxt)
        return []


# è®­ç»ƒç»Ÿè®¡ï¼ˆå…¨å±€å˜é‡ï¼Œç”¨äºtrain_episodeå‡½æ•°ï¼‰
training_stats = {
    'episode': 0,
    'total_reward': 0,
    'episode_rewards': [],
    'losses': [],
    'epsilon_history': [],
    'wins': 0,
    'losses_count': 0,
    'draws': 0
}


def train_episode(l_agent, r_agent, simulator, l_world_wrapper, r_world_wrapper):
    """è®­ç»ƒä¸€ä¸ªepisode - Lé˜Ÿå’ŒRé˜Ÿéƒ½ä½¿ç”¨RLç­–ç•¥"""
    global training_stats
    
    simulator.reset()
    l_prev_states = {}
    r_prev_states = {}
    l_episode_reward = 0
    r_episode_reward = 0
    
    while not simulator.step():
        # è·å–Lé˜Ÿç©å®¶ï¼ˆè®­ç»ƒagentï¼‰
        l_players = simulator.list_players(mine=True, inPrison=None, hasFlag=None)
        
        actions = {}
        
        # Lé˜Ÿç©å®¶å†³ç­–ï¼ˆä½¿ç”¨L agentï¼‰
        for player in l_players:
            if player["inPrison"]:
                continue
            
            player_name = player["name"]
            
            # æå–çŠ¶æ€ï¼ˆä½¿ç”¨Lé˜Ÿè§†è§’çš„world wrapperï¼‰
            current_state = RL.extract_state_features(player, l_world_wrapper)
            
            # é€‰æ‹©åŠ¨ä½œ
            action_idx = l_agent.select_action(current_state, training=True)
            
            # è·å–ä¸Šä¸€å¸§çŠ¶æ€
            prev_state_dict = l_prev_states.get(player_name)
            
            # è®¡ç®—å¥–åŠ±
            reward = l_agent.calculate_reward(player, l_world_wrapper, prev_state_dict)
            l_episode_reward += reward
            
            # å­˜å‚¨ç»éªŒ
            if prev_state_dict is not None:
                prev_player = {
                    "posX": prev_state_dict["posX"],
                    "posY": prev_state_dict["posY"],
                    "hasFlag": prev_state_dict["hasFlag"],
                    "inPrison": prev_state_dict["inPrison"],
                    "team": player.get("team", ""),
                    "name": player_name
                }
                prev_state = RL.extract_state_features(prev_player, l_world_wrapper)
                
                done = player.get("inPrison", False)
                
                l_agent.replay_buffer.push(
                    prev_state,
                    action_idx,
                    reward,
                    current_state,
                    done
                )
            
            # æ‰§è¡ŒåŠ¨ä½œ - ä½¿ç”¨çœŸå®è·¯å¾„è§„åˆ’
            action_map = {0: "defence", 1: "scoring", 2: "saving"}
            action_type = action_map[action_idx]
            
            direction = ""
            
            # ä½¿ç”¨çœŸå®è·¯å¾„è§„åˆ’ï¼ˆå¿…éœ€ï¼‰
            if action_type == "scoring":
                # æ‰¾æœ€è¿‘çš„flag
                flags = simulator.list_flags(mine=False, canPickup=True)
                target_flag = flags[0] if flags else None
                direction = pf.scoring(l_world_wrapper, player, target_flag)
                
            elif action_type == "defence":
                # æ‰¾æœ€è¿‘çš„æ•Œäºº
                enemies = simulator.list_players(mine=False, inPrison=False, hasFlag=None)
                if enemies:
                    enemy = enemies[0]
                    direction = pf.defence(l_world_wrapper, player, enemy)
                    
            elif action_type == "saving":
                direction = pf.saving(l_world_wrapper, player)
            
            if direction:
                simulator.apply_action(player_name, direction)
            
            # æ›´æ–°ä¸Šä¸€å¸§çŠ¶æ€
            l_prev_states[player_name] = {
                "hasFlag": player.get("hasFlag", False),
                "inPrison": player.get("inPrison", False),
                "posX": player["posX"],
                "posY": player["posY"]
            }
        
        # Ré˜Ÿç©å®¶å†³ç­–ï¼ˆä½¿ç”¨R agentï¼Œç›¸åŒç­–ç•¥ï¼‰
        r_players = simulator.list_players(mine=False, inPrison=None, hasFlag=None)
        for player in r_players:
            if player["inPrison"]:
                continue
            
            player_name = player["name"]
            
            # æå–çŠ¶æ€ï¼ˆä½¿ç”¨Ré˜Ÿè§†è§’çš„world wrapperï¼‰
            current_state = RL.extract_state_features(player, r_world_wrapper)
            
            # é€‰æ‹©åŠ¨ä½œ
            action_idx = r_agent.select_action(current_state, training=True)
            
            # è·å–ä¸Šä¸€å¸§çŠ¶æ€
            prev_state_dict = r_prev_states.get(player_name)
            
            # è®¡ç®—å¥–åŠ±
            reward = r_agent.calculate_reward(player, r_world_wrapper, prev_state_dict)
            r_episode_reward += reward
            
            # å­˜å‚¨ç»éªŒ
            if prev_state_dict is not None:
                prev_player = {
                    "posX": prev_state_dict["posX"],
                    "posY": prev_state_dict["posY"],
                    "hasFlag": prev_state_dict["hasFlag"],
                    "inPrison": prev_state_dict["inPrison"],
                    "team": player.get("team", ""),
                    "name": player_name
                }
                prev_state = RL.extract_state_features(prev_player, r_world_wrapper)
                
                done = player.get("inPrison", False)
                
                r_agent.replay_buffer.push(
                    prev_state,
                    action_idx,
                    reward,
                    current_state,
                    done
                )
            
            # æ‰§è¡ŒåŠ¨ä½œ - ä½¿ç”¨çœŸå®è·¯å¾„è§„åˆ’ï¼ˆä¸Lé˜Ÿç›¸åŒï¼‰
            action_map = {0: "defence", 1: "scoring", 2: "saving"}
            action_type = action_map[action_idx]
            
            direction = ""
            
            # ä½¿ç”¨çœŸå®è·¯å¾„è§„åˆ’ï¼ˆå¿…éœ€ï¼‰
            if action_type == "scoring":
                flags = simulator.list_flags(mine=True, canPickup=True)
                target_flag = flags[0] if flags else None
                direction = pf.scoring(r_world_wrapper, player, target_flag)
                
            elif action_type == "defence":
                enemies = simulator.list_players(mine=True, inPrison=False, hasFlag=None)
                if enemies:
                    enemy = enemies[0]
                    direction = pf.defence(r_world_wrapper, player, enemy)
                    
            elif action_type == "saving":
                direction = pf.saving(r_world_wrapper, player)
            
            if direction:
                simulator.apply_action(player_name, direction)
            
            # æ›´æ–°ä¸Šä¸€å¸§çŠ¶æ€
            r_prev_states[player_name] = {
                "hasFlag": player.get("hasFlag", False),
                "inPrison": player.get("inPrison", False),
                "posX": player["posX"],
                "posY": player["posY"]
            }
        
        # è®­ç»ƒï¼ˆæ¯5æ­¥ï¼‰- ä¸¤ä¸ªagentéƒ½è®­ç»ƒ
        if len(l_agent.replay_buffer) >= 32 and len(l_agent.replay_buffer) % 5 == 0:
            loss = l_agent.train_step(batch_size=32)
            if loss is not None:
                training_stats['losses'].append(loss)
        
        if len(r_agent.replay_buffer) >= 32 and len(r_agent.replay_buffer) % 5 == 0:
            loss = r_agent.train_step(batch_size=32)
            if loss is not None:
                training_stats['losses'].append(loss)
    
    # Episodeç»“æŸ
    training_stats['episode'] += 1
    training_stats['total_reward'] = l_episode_reward  # ä½¿ç”¨Lé˜Ÿçš„å¥–åŠ±ä½œä¸ºä¸»è¦æŒ‡æ ‡
    training_stats['episode_rewards'].append(l_episode_reward)
    training_stats['epsilon_history'].append(l_agent.epsilon)
    
    # åˆ¤æ–­èƒœè´Ÿ
    if simulator.l_score > simulator.r_score:
        training_stats['wins'] += 1
        result = "WIN"
    elif simulator.l_score < simulator.r_score:
        training_stats['losses_count'] += 1
        result = "LOSS"
    else:
        training_stats['draws'] += 1
        result = "DRAW"
    
    l_agent.update_epsilon()
    r_agent.update_epsilon()
    
    return result, simulator.l_score, simulator.r_score


def worker_process(worker_id, model_queue, experience_queue, num_episodes_per_worker, state_dim, action_dim, device):
    """Workerè¿›ç¨‹ï¼šå¹¶è¡Œè¿è¡Œepisodeæ”¶é›†ç»éªŒ"""
    # æ¯ä¸ªworkeråˆ›å»ºè‡ªå·±çš„agentå’Œæ¨¡æ‹Ÿå™¨ï¼ˆå¯ç”¨Double DQNï¼Œä½¿ç”¨ç›¸åŒè¶…å‚æ•°ï¼‰
    l_agent = RL.DQNAgent(state_dim, action_dim,
                          lr=0.0005,
                          epsilon_decay=0.9995,
                          epsilon_end=0.05,
                          device=device, 
                          use_double_dqn=True)
    r_agent = RL.DQNAgent(state_dim, action_dim,
                          lr=0.0005,
                          epsilon_decay=0.9995,
                          epsilon_end=0.05,
                          device=device, 
                          use_double_dqn=True)
    
    simulator = SimpleGameSimulator(width=20, height=20, num_players=3, num_flags=9, 
                                    num_obstacles_1=8, num_obstacles_2=4)
    l_world_wrapper = SimpleWorldWrapper(simulator, team="L")
    r_world_wrapper = SimpleWorldWrapper(simulator, team="R")
    
    episode_count = 0
    
    while episode_count < num_episodes_per_worker:
        # å°è¯•ä»é˜Ÿåˆ—è·å–æœ€æ–°æ¨¡å‹ï¼ˆéé˜»å¡ï¼‰
        # æ³¨æ„ï¼šä»0è®­ç»ƒæ—¶ï¼Œworkerä¹Ÿä¼šæ”¶åˆ°æ–°è®­ç»ƒçš„æ¨¡å‹ï¼Œè¿™æ˜¯æ­£å¸¸çš„
        # æ³¨æ„ï¼šworkeråŠ è½½æ¨¡å‹æ—¶ï¼Œæ¨¡å‹ä¼šä»CUDAè‡ªåŠ¨è½¬æ¢åˆ°CPUï¼ˆå¦‚æœä¸»è¿›ç¨‹ä½¿ç”¨CUDAï¼‰
        try:
            model_path = model_queue.get_nowait()
            l_agent.load_model(model_path)
            # Ré˜Ÿä¹ŸåŠ è½½æ¨¡å‹ï¼Œä½†ä¿æŒæ›´é«˜çš„æ¢ç´¢ç‡
            r_agent.load_model(model_path)
            r_agent.epsilon = max(r_agent.epsilon, 0.15)  # ç¡®ä¿Ré˜Ÿä¿æŒé«˜æ¢ç´¢
        except:
            pass  # é˜Ÿåˆ—ä¸ºç©ºï¼Œä½¿ç”¨å½“å‰æ¨¡å‹
        
        # è¿è¡Œä¸€ä¸ªepisode
        result, l_score, r_score, experiences = train_episode_parallel(
            l_agent, r_agent, simulator, l_world_wrapper, r_world_wrapper
        )
        
        # å°†ç»éªŒå‘é€å›ä¸»è¿›ç¨‹
        experience_queue.put({
            'worker_id': worker_id,
            'episode': episode_count,
            'result': result,
            'l_score': l_score,
            'r_score': r_score,
            'experiences': experiences
        })
        
        episode_count += 1


def train_episode_parallel(l_agent, r_agent, simulator, l_world_wrapper, r_world_wrapper):
    """è®­ç»ƒä¸€ä¸ªepisodeå¹¶è¿”å›ç»éªŒï¼ˆç”¨äºå¹¶è¡Œè®­ç»ƒï¼‰"""
    simulator.reset()
    l_prev_states = {}
    r_prev_states = {}
    l_episode_reward = 0
    r_episode_reward = 0
    experiences = []  # æ”¶é›†æ‰€æœ‰ç»éªŒ
    
    while not simulator.step():
        # Lé˜Ÿç©å®¶å†³ç­–
        l_players = simulator.list_players(mine=True, inPrison=None, hasFlag=None)
        for player in l_players:
            if player["inPrison"]:
                continue
            
            player_name = player["name"]
            current_state = RL.extract_state_features(player, l_world_wrapper)
            action_idx = l_agent.select_action(current_state, training=True)
            prev_state_dict = l_prev_states.get(player_name)
            reward = l_agent.calculate_reward(player, l_world_wrapper, prev_state_dict, current_action=action_idx)
            l_episode_reward += reward
            
            if prev_state_dict is not None:
                prev_player = {
                    "posX": prev_state_dict["posX"],
                    "posY": prev_state_dict["posY"],
                    "hasFlag": prev_state_dict["hasFlag"],
                    "inPrison": prev_state_dict["inPrison"],
                    "team": player.get("team", ""),
                    "name": player_name
                }
                prev_state = RL.extract_state_features(prev_player, l_world_wrapper)
                done = player.get("inPrison", False)
                
                experiences.append(('l', prev_state, action_idx, reward, current_state, done))
            
            # æ‰§è¡ŒåŠ¨ä½œ - ä½¿ç”¨çœŸå®è·¯å¾„è§„åˆ’
            action_map = {0: "defence", 1: "scoring", 2: "saving"}
            action_type = action_map[action_idx]
            
            direction = ""
            
            # ä½¿ç”¨çœŸå®è·¯å¾„è§„åˆ’ï¼ˆå¿…éœ€ï¼‰
            if action_type == "scoring":
                flags = simulator.list_flags(mine=False, canPickup=True)
                target_flag = flags[0] if flags else None
                direction = pf.scoring(l_world_wrapper, player, target_flag)
            elif action_type == "defence":
                enemies = simulator.list_players(mine=False, inPrison=False, hasFlag=None)
                if enemies:
                    enemy = enemies[0]
                    direction = pf.defence(l_world_wrapper, player, enemy)
            elif action_type == "saving":
                direction = pf.saving(l_world_wrapper, player)
            
            if direction:
                simulator.apply_action(player_name, direction)
            
            l_prev_states[player_name] = {
                "hasFlag": player.get("hasFlag", False),
                "inPrison": player.get("inPrison", False),
                "posX": player["posX"],
                "posY": player["posY"]
            }
        
        # Ré˜Ÿç©å®¶å†³ç­–
        r_players = simulator.list_players(mine=False, inPrison=None, hasFlag=None)
        for player in r_players:
            if player["inPrison"]:
                continue
            
            player_name = player["name"]
            current_state = RL.extract_state_features(player, r_world_wrapper)
            action_idx = r_agent.select_action(current_state, training=True)
            prev_state_dict = r_prev_states.get(player_name)
            reward = r_agent.calculate_reward(player, r_world_wrapper, prev_state_dict, current_action=action_idx)
            r_episode_reward += reward
            
            if prev_state_dict is not None:
                prev_player = {
                    "posX": prev_state_dict["posX"],
                    "posY": prev_state_dict["posY"],
                    "hasFlag": prev_state_dict["hasFlag"],
                    "inPrison": prev_state_dict["inPrison"],
                    "team": player.get("team", ""),
                    "name": player_name
                }
                prev_state = RL.extract_state_features(prev_player, r_world_wrapper)
                done = player.get("inPrison", False)
                
                experiences.append(('r', prev_state, action_idx, reward, current_state, done))
            
            # æ‰§è¡ŒåŠ¨ä½œ - ä½¿ç”¨çœŸå®è·¯å¾„è§„åˆ’
            action_map = {0: "defence", 1: "scoring", 2: "saving"}
            action_type = action_map[action_idx]
            
            direction = ""
            
            # ä½¿ç”¨çœŸå®è·¯å¾„è§„åˆ’ï¼ˆå¿…éœ€ï¼‰
            if action_type == "scoring":
                flags = simulator.list_flags(mine=True, canPickup=True)
                target_flag = flags[0] if flags else None
                direction = pf.scoring(r_world_wrapper, player, target_flag)
            elif action_type == "defence":
                enemies = simulator.list_players(mine=True, inPrison=False, hasFlag=None)
                if enemies:
                    enemy = enemies[0]
                    direction = pf.defence(r_world_wrapper, player, enemy)
            elif action_type == "saving":
                direction = pf.saving(r_world_wrapper, player)
            
            if direction:
                simulator.apply_action(player_name, direction)
            
            r_prev_states[player_name] = {
                "hasFlag": player.get("hasFlag", False),
                "inPrison": player.get("inPrison", False),
                "posX": player["posX"],
                "posY": player["posY"]
            }
    
    # åˆ¤æ–­èƒœè´Ÿ
    if simulator.l_score > simulator.r_score:
        result = "WIN"
    elif simulator.l_score < simulator.r_score:
        result = "LOSS"
    else:
        result = "DRAW"
    
    return result, simulator.l_score, simulator.r_score, experiences


def evaluate_model(current_agent, best_agent, num_games=10):
    """
    è¯„ä¼°æ¨¡å‹ï¼šè¿è¡ŒæŒ‡å®šæ•°é‡çš„æ¯”èµ›ï¼Œè¿”å›èƒœç‡
    Args:
        current_agent: å½“å‰è¦è¯„ä¼°çš„æ¨¡å‹
        best_agent: å†å²æœ€ä½³æ¨¡å‹ï¼ˆä½œä¸ºå¯¹æ‰‹ï¼‰
        num_games: è¯„ä¼°æ¯”èµ›æ•°é‡
    Returns:
        (win_rate, wins, losses, draws): èƒœç‡ã€èƒœåˆ©æ•°ã€å¤±è´¥æ•°ã€å¹³å±€æ•°
    """
    state_dim = 19
    action_dim = 3
    
    # åˆ›å»ºè¯„ä¼°ç”¨çš„æ¨¡æ‹Ÿå™¨å’Œworld wrapper
    simulator = SimpleGameSimulator(width=20, height=20, num_players=3, num_flags=9, 
                                    num_obstacles_1=8, num_obstacles_2=4)
    l_world_wrapper = SimpleWorldWrapper(simulator, team="L")
    r_world_wrapper = SimpleWorldWrapper(simulator, team="R")
    
    # åˆ›å»ºè¯„ä¼°ç”¨çš„å¯¹æ‰‹agentï¼ˆä½¿ç”¨best_agentçš„æ¨¡å‹ï¼‰
    eval_opponent = RL.DQNAgent(state_dim, action_dim, device=current_agent.device, use_double_dqn=True)
    eval_opponent.q_network.load_state_dict(best_agent.q_network.state_dict())
    eval_opponent.target_network.load_state_dict(best_agent.target_network.state_dict())
    eval_opponent.epsilon = 0.0  # è¯„ä¼°æ—¶ä¸æ¢ç´¢ï¼Œä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
    
    # å½“å‰æ¨¡å‹ä¹Ÿè®¾ç½®ä¸ºä¸æ¢ç´¢
    current_agent_eval = RL.DQNAgent(state_dim, action_dim, device=current_agent.device, use_double_dqn=True)
    current_agent_eval.q_network.load_state_dict(current_agent.q_network.state_dict())
    current_agent_eval.target_network.load_state_dict(current_agent.target_network.state_dict())
    current_agent_eval.epsilon = 0.0  # è¯„ä¼°æ—¶ä¸æ¢ç´¢
    
    wins = 0
    losses = 0
    draws = 0
    
    # è¿è¡Œè¯„ä¼°æ¯”èµ›
    for game_idx in range(num_games):
        simulator.reset()
        
        while not simulator.step():
            # Lé˜Ÿï¼ˆå½“å‰æ¨¡å‹ï¼‰å†³ç­–
            l_players = simulator.list_players(mine=True, inPrison=None, hasFlag=None)
            for player in l_players:
                if player["inPrison"]:
                    continue
                
                player_name = player["name"]
                current_state = RL.extract_state_features(player, l_world_wrapper)
                action_idx = current_agent_eval.select_action(current_state, training=False)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                action_map = {0: "defence", 1: "scoring", 2: "saving"}
                action_type = action_map[action_idx]
                direction = ""
                
                try:
                    import pathfinding_adapter as pf
                    if action_type == "scoring":
                        flags = simulator.list_flags(mine=False, canPickup=True)
                        target_flag = flags[0] if flags else None
                        if target_flag:
                            direction = pf.scoring(l_world_wrapper, player, target_flag)
                    elif action_type == "defence":
                        enemies = simulator.list_players(mine=False, inPrison=False, hasFlag=None)
                        if enemies:
                            enemy = enemies[0]
                            direction = pf.defence(l_world_wrapper, player, enemy)
                    elif action_type == "saving":
                        direction = pf.saving(l_world_wrapper, player)
                except:
                    pass
                
                if direction:
                    simulator.apply_action(player_name, direction)
            
            # Ré˜Ÿï¼ˆæœ€ä½³æ¨¡å‹ï¼‰å†³ç­–
            r_players = simulator.list_players(mine=False, inPrison=None, hasFlag=None)
            for player in r_players:
                if player["inPrison"]:
                    continue
                
                player_name = player["name"]
                current_state = RL.extract_state_features(player, r_world_wrapper)
                action_idx = eval_opponent.select_action(current_state, training=False)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                action_map = {0: "defence", 1: "scoring", 2: "saving"}
                action_type = action_map[action_idx]
                direction = ""
                
                try:
                    import pathfinding_adapter as pf
                    if action_type == "scoring":
                        flags = simulator.list_flags(mine=True, canPickup=True)
                        target_flag = flags[0] if flags else None
                        if target_flag:
                            direction = pf.scoring(r_world_wrapper, player, target_flag)
                    elif action_type == "defence":
                        enemies = simulator.list_players(mine=True, inPrison=False, hasFlag=None)
                        if enemies:
                            enemy = enemies[0]
                            direction = pf.defence(r_world_wrapper, player, enemy)
                    elif action_type == "saving":
                        direction = pf.saving(r_world_wrapper, player)
                except:
                    pass
                
                if direction:
                    simulator.apply_action(player_name, direction)
        
        # åˆ¤æ–­èƒœè´Ÿ
        if simulator.l_score > simulator.r_score:
            wins += 1
        elif simulator.l_score < simulator.r_score:
            losses += 1
        else:
            draws += 1
    
    win_rate = wins / num_games * 100
    return win_rate, wins, losses, draws


def main():
    """ä¸»å‡½æ•° - å¹¶è¡Œè®­ç»ƒç‰ˆæœ¬"""
    print("=" * 60)
    print("å¹¶è¡Œè®­ç»ƒæ¨¡å¼ - å¤šè¿›ç¨‹åŠ é€Ÿ")
    print("Lé˜Ÿå’ŒRé˜Ÿä½¿ç”¨ç›¸åŒRLç­–ç•¥å¯¹æ‰“")
    print("=" * 60)
    
    # ========== è®­ç»ƒé…ç½®é€‰é¡¹ ==========
    # æ˜¯å¦ä»å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆFalse=ä»0å¼€å§‹ï¼ŒTrue=ä»å·²æœ‰æ¨¡å‹ç»§ç»­ï¼‰
    LOAD_EXISTING_MODEL = False # è®¾ç½®ä¸ºFalseä»0å¼€å§‹è®­ç»ƒï¼ŒTrueä»å·²æœ‰æ¨¡å‹ç»§ç»­
    
    # é…ç½®
    num_workers = multiprocessing.cpu_count()  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
    print(f"ä½¿ç”¨ {num_workers} ä¸ªworkerè¿›ç¨‹å¹¶è¡Œè®­ç»ƒ")
    
    state_dim = 19
    action_dim = 3
    
    # è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨CUDAï¼ˆå¦‚æœå¯ç”¨ï¼‰
    import torch
    if torch.cuda.is_available():
        device = 'cuda'
        print(f"âœ… ä½¿ç”¨CUDAåŠ é€Ÿè®­ç»ƒ (è®¾å¤‡: {torch.cuda.get_device_name(0)})")
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
    else:
        device = 'cpu'
        print("â„¹ï¸  ä½¿ç”¨CPUè®­ç»ƒï¼ˆCUDAä¸å¯ç”¨ï¼‰")
    
    if LOAD_EXISTING_MODEL:
        print("âœ… è®­ç»ƒæ¨¡å¼: ä»å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ")
    else:
        print("ğŸ†• è®­ç»ƒæ¨¡å¼: ä»0å¼€å§‹è®­ç»ƒï¼ˆä¸åŠ è½½å·²æœ‰æ¨¡å‹ï¼‰")
    
    # åˆ›å»ºä¸»agentï¼ˆå¯ç”¨Double DQNï¼Œè°ƒæ•´è¶…å‚æ•°ï¼‰
    # Lé˜Ÿï¼šå½“å‰æœ€æ–°ç­–ç•¥ï¼ˆè®­ç»ƒç›®æ ‡ï¼‰
    l_agent = RL.DQNAgent(state_dim, action_dim, 
                          lr=0.0005,  # é™ä½å­¦ä¹ ç‡ï¼ˆä»0.001é™åˆ°0.0005ï¼‰
                          epsilon_decay=0.9995,  # å‡æ…¢epsilonè¡°å‡ï¼ˆä»0.995åˆ°0.9995ï¼‰
                          epsilon_end=0.05,  # æé«˜æœ€å°epsilonï¼ˆä»0.01åˆ°0.05ï¼Œä¿æŒæ›´å¤šæ¢ç´¢ï¼‰
                          device=device, 
                          use_double_dqn=True)
    
    # Ré˜Ÿï¼šä½¿ç”¨æ›´é«˜çš„æ¢ç´¢ç‡ï¼Œå¢åŠ ç­–ç•¥å¤šæ ·æ€§
    r_agent = RL.DQNAgent(state_dim, action_dim,
                          lr=0.0005,
                          epsilon_decay=0.9995,
                          epsilon_end=0.15,  # Ré˜Ÿä¿æŒæ›´é«˜æ¢ç´¢ï¼ˆ0.15 vs 0.05ï¼‰
                          epsilon_start=1.0,  # é‡æ–°å¼€å§‹æ¢ç´¢
                          device=device, 
                          use_double_dqn=True)
    r_agent.epsilon = 0.2  # Ré˜Ÿåˆå§‹epsilonæ›´é«˜ï¼Œä¿æŒæ›´å¤šæ¢ç´¢
    
    print("âœ… ä½¿ç”¨Double DQNç®—æ³•ï¼ˆå‡å°‘Qå€¼è¿‡ä¼°è®¡ï¼‰")
    print("âœ… è¶…å‚æ•°è°ƒæ•´ï¼šlr=0.0005, epsilon_decay=0.9995")
    print("âœ… ç­–ç•¥å¤šæ ·æ€§ï¼šLé˜Ÿepsilon_end=0.05, Ré˜Ÿepsilon_end=0.15ï¼ˆæ›´é«˜æ¢ç´¢ï¼‰")
    
    # åˆå§‹åŒ–æœ€ä½³æ¨¡å‹ï¼ˆç”¨äºè¯„ä¼°å¯¹æ¯”ï¼‰
    best_model_path = "lib/models/dqn_model_final.pth"
    best_win_rate = 0.0  # å†å²æœ€ä½³èƒœç‡
    best_agent = RL.DQNAgent(state_dim, action_dim, device=device, use_double_dqn=True)
    if os.path.exists(best_model_path):
        best_agent.load_model(best_model_path)
        print(f"âœ… åŠ è½½å†å²æœ€ä½³æ¨¡å‹: {best_model_path}")
    else:
        # åˆå§‹æ—¶ï¼Œå½“å‰æ¨¡å‹å°±æ˜¯æœ€ä½³æ¨¡å‹
        best_agent.q_network.load_state_dict(l_agent.q_network.state_dict())
        best_agent.target_network.load_state_dict(l_agent.target_network.state_dict())
        print(f"ğŸ†• åˆå§‹åŒ–æœ€ä½³æ¨¡å‹ï¼ˆä½¿ç”¨å½“å‰æ¨¡å‹ï¼‰")
    
    # åŠ è½½æ¨¡å‹ï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦åŠ è½½ï¼‰
    model_path = "./lib/models/dqn_model_latest.pth"
    if LOAD_EXISTING_MODEL and os.path.exists(model_path):
        l_agent.load_model(model_path)
        r_agent.load_model(model_path)
        # Ré˜ŸåŠ è½½åä¿æŒé«˜æ¢ç´¢ç‡ï¼ˆç­–ç•¥å¤šæ ·æ€§ï¼‰
        r_agent.epsilon = max(r_agent.epsilon, 0.2)
        print(f"âœ… åŠ è½½å·²æœ‰æ¨¡å‹: {model_path}")
        print(f"  Ré˜Ÿåˆå§‹epsilon: {r_agent.epsilon:.3f} (ä¿æŒé«˜æ¢ç´¢ä»¥å®ç°ç­–ç•¥å¤šæ ·æ€§)")
    else:
        if LOAD_EXISTING_MODEL:
            print(f"âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        else:
            print("ğŸ†• ä»0å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        # ç¡®ä¿Ré˜Ÿåˆå§‹é«˜æ¢ç´¢
        r_agent.epsilon = 0.2
    
    # åˆ›å»ºé˜Ÿåˆ—
    model_queue = Queue()  # ç”¨äºå‘workerå‘é€æ¨¡å‹æ›´æ–°
    experience_queue = Queue()  # ç”¨äºæ¥æ”¶workerçš„ç»éªŒ
    
    # å¯¹æ‰‹æ± ï¼šå­˜å‚¨å†å²æ¨¡å‹è·¯å¾„ï¼ˆæ¯500ä¸ªepisodeä¿å­˜ä¸€æ¬¡ï¼‰
    opponent_pool = []
    opponent_pool_dir = "lib/models/opponent_pool"
    os.makedirs(opponent_pool_dir, exist_ok=True)
    
    # åŠ è½½å·²æœ‰çš„å¯¹æ‰‹æ± æ¨¡å‹
    if os.path.exists(opponent_pool_dir):
        existing_models = sorted([f for f in os.listdir(opponent_pool_dir) if f.endswith('.pth')])
        for model_file in existing_models[-10:]:  # åªåŠ è½½æœ€è¿‘10ä¸ª
            opponent_pool.append(os.path.join(opponent_pool_dir, model_file))
    
    print(f"âœ… å¯¹æ‰‹æ± å·²åˆå§‹åŒ–: {opponent_pool_dir} (å·²æœ‰ {len(opponent_pool)} ä¸ªæ¨¡å‹)")
    
    # ç­–ç•¥å¤šæ ·æ€§é…ç½®
    strategy_diversity_config = {
        'use_opponent_pool_prob': 0.5,  # 50%æ¦‚ç‡ä½¿ç”¨å¯¹æ‰‹æ± 
        'use_high_exploration_prob': 0.3,  # 30%æ¦‚ç‡ä½¿ç”¨é«˜æ¢ç´¢ç‡
        'use_current_model_prob': 0.2,  # 20%æ¦‚ç‡ä½¿ç”¨å½“å‰æ¨¡å‹ï¼ˆä½†ä¸åŒepsilonï¼‰
        'opponent_switch_freq': 5,  # æ¯5ä¸ªepisodeåˆ‡æ¢ä¸€æ¬¡ç­–ç•¥
    }
    last_strategy_switch = 0
    current_r_strategy = 'current'  # 'current', 'opponent_pool', 'high_exploration'
    
    # å¯åŠ¨workerè¿›ç¨‹
    workers = []
    for i in range(num_workers):
        p = Process(target=worker_process, args=(
            i, model_queue, experience_queue, 
            1000,  # æ¯ä¸ªworkerè¿è¡Œ1000ä¸ªepisodeï¼ˆæˆ–ç›´åˆ°ä¸»è¿›ç¨‹åœæ­¢ï¼‰
            state_dim, action_dim, device
        ))
        p.start()
        workers.append(p)
        print(f"å¯åŠ¨worker {i}")
    
    # è®­ç»ƒé…ç½®
    MAX_EPISODES = 10000  # æœ€å¤§è®­ç»ƒæ¬¡æ•°
    TARGET_WIN_RATE = 80.0  # ç›®æ ‡èƒœç‡
    
    print(f"\nå¼€å§‹é•¿æœŸå¹¶è¡Œè®­ç»ƒ...")
    print(f"ğŸ¯ è®­ç»ƒç›®æ ‡ï¼šèƒœç‡ >= {TARGET_WIN_RATE}%")
    print(f"ğŸ“Š æœ€å¤§è®­ç»ƒæ¬¡æ•°ï¼š{MAX_EPISODES} episodes")
    print(f"ğŸ’¾ ä¿å­˜é¢‘ç‡ï¼šæ¯1000ä¸ªepisode")
    print(f"â¸ï¸  æŒ‰ Ctrl+C å¯æ‰‹åŠ¨åœæ­¢")
    print("=" * 60)
    
    # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡ï¼ˆåœ¨mainå‡½æ•°ä¸­ï¼‰
    training_stats = {
        'episode': 0,
        'total_reward': 0,
        'episode_rewards': [],
        'losses': [],
        'epsilon_history': [],
        'wins': 0,
        'losses_count': 0,
        'draws': 0
    }
    
    # è¯„ä¼°çª—å£ï¼šç”¨äºè®¡ç®—æœ€è¿‘10ä¸ªepisodeçš„èƒœç‡
    evaluation_window = []  # å­˜å‚¨æœ€è¿‘10ä¸ªepisodeçš„ç»“æœ ["WIN", "LOSS", "DRAW", ...]
    EVALUATION_WINDOW_SIZE = 10  # è¯„ä¼°çª—å£å¤§å°
    
    try:
        episode = 0
        while True:
            # æ”¶é›†ç»éªŒ
            batch_experiences = []
            for _ in range(num_workers):  # ä»æ¯ä¸ªworkeræ”¶é›†ä¸€ä¸ªepisode
                try:
                    data = experience_queue.get(timeout=10)
                    batch_experiences.append(data)
                except:
                    continue
            
            # å°†ç»éªŒæ·»åŠ åˆ°ä¸»agentçš„replay buffer
            for data in batch_experiences:
                for exp in data['experiences']:
                    team, prev_state, action, reward, next_state, done = exp
                    if team == 'l':
                        l_agent.replay_buffer.push(prev_state, action, reward, next_state, done)
                    else:
                        r_agent.replay_buffer.push(prev_state, action, reward, next_state, done)
                
                # æ›´æ–°ç»Ÿè®¡
                episode += 1
                training_stats['episode'] = episode
                if data['result'] == "WIN":
                    training_stats['wins'] += 1
                elif data['result'] == "LOSS":
                    training_stats['losses_count'] += 1
                else:
                    training_stats['draws'] += 1
                
                # æ›´æ–°è¯„ä¼°çª—å£ï¼ˆç”¨äºè®¡ç®—æœ€è¿‘10ä¸ªepisodeçš„èƒœç‡ï¼‰
                evaluation_window.append(data['result'])
                if len(evaluation_window) > EVALUATION_WINDOW_SIZE:
                    evaluation_window.pop(0)  # ä¿æŒçª—å£å¤§å°ä¸º10
            
            # è®­ç»ƒ
            if len(l_agent.replay_buffer) >= 32:
                for _ in range(min(10, len(batch_experiences))):  # è®­ç»ƒå¤šæ¬¡
                    loss = l_agent.train_step(batch_size=32)
                    if loss is not None:
                        training_stats['losses'].append(loss)
                
                for _ in range(min(10, len(batch_experiences))):
                    loss = r_agent.train_step(batch_size=32)
                    if loss is not None:
                        training_stats['losses'].append(loss)
            
            # ç­–ç•¥å¤šæ ·æ€§ï¼šå®šæœŸåˆ‡æ¢Ré˜Ÿçš„ç­–ç•¥
            if episode - last_strategy_switch >= strategy_diversity_config['opponent_switch_freq']:
                rand = random.random()
                if rand < strategy_diversity_config['use_opponent_pool_prob'] and len(opponent_pool) > 0:
                    # ä½¿ç”¨å¯¹æ‰‹æ± ä¸­çš„å†å²æ¨¡å‹
                    selected_opponent = random.choice(opponent_pool)
                    try:
                        r_agent.load_model(selected_opponent)
                        # ä¿æŒè¾ƒé«˜çš„æ¢ç´¢ç‡
                        r_agent.epsilon = max(r_agent.epsilon, 0.15)
                        current_r_strategy = 'opponent_pool'
                        print(f"  ğŸ¯ Ré˜Ÿç­–ç•¥åˆ‡æ¢: ä½¿ç”¨å†å²å¯¹æ‰‹ {os.path.basename(selected_opponent)} (epsilon={r_agent.epsilon:.3f})")
                    except Exception as e:
                        print(f"  âš ï¸  åŠ è½½å¯¹æ‰‹æ¨¡å‹å¤±è´¥: {e}")
                        current_r_strategy = 'current'
                elif rand < strategy_diversity_config['use_opponent_pool_prob'] + strategy_diversity_config['use_high_exploration_prob']:
                    # ä½¿ç”¨å½“å‰æ¨¡å‹ä½†æé«˜æ¢ç´¢ç‡
                    latest_path = "lib/models/dqn_model_latest.pth"
                    if os.path.exists(latest_path):
                        r_agent.load_model(latest_path)
                    r_agent.epsilon = max(r_agent.epsilon, 0.2)  # å¼ºåˆ¶é«˜æ¢ç´¢
                    current_r_strategy = 'high_exploration'
                    print(f"  ğŸ¯ Ré˜Ÿç­–ç•¥åˆ‡æ¢: é«˜æ¢ç´¢æ¨¡å¼ (epsilon={r_agent.epsilon:.3f})")
                else:
                    # ä½¿ç”¨å½“å‰æ¨¡å‹ï¼Œä½†ä¿æŒä¸åŒepsilon
                    latest_path = "lib/models/dqn_model_latest.pth"
                    if os.path.exists(latest_path):
                        r_agent.load_model(latest_path)
                    r_agent.epsilon = max(r_agent.epsilon, 0.1)  # ä¸­ç­‰æ¢ç´¢
                    current_r_strategy = 'current'
                    print(f"  ğŸ¯ Ré˜Ÿç­–ç•¥åˆ‡æ¢: å½“å‰æ¨¡å‹ (epsilon={r_agent.epsilon:.3f})")
                
                last_strategy_switch = episode
            
            # å®šæœŸåŒæ­¥Lé˜Ÿæ¨¡å‹åˆ°workerï¼ˆæ— è®ºæ˜¯å¦ä»0è®­ç»ƒï¼Œéƒ½ä¿å­˜æ¨¡å‹ä¾›åç»­ä½¿ç”¨ï¼‰
            if episode % 10 == 0:
                latest_path = "lib/models/dqn_model_latest.pth"
                l_agent.save_model(latest_path)
                # å‘æ‰€æœ‰workerå‘é€æ¨¡å‹æ›´æ–°ï¼ˆä»0è®­ç»ƒæ—¶ï¼Œworkerä¹Ÿä¼šæ”¶åˆ°æ–°è®­ç»ƒçš„æ¨¡å‹ï¼‰
                for _ in range(num_workers):
                    model_queue.put(latest_path)
            
            # æ›´æ–°epsilonï¼ˆä½†Ré˜Ÿä¿æŒæ›´é«˜æ¢ç´¢ï¼‰
            if episode % 10 == 0:
                l_agent.update_epsilon()
                # Ré˜Ÿä¹Ÿæ›´æ–°epsilonï¼Œä½†ç¡®ä¿ä¸ä½äºæœ€å°å€¼
                r_agent.update_epsilon()
                r_agent.epsilon = max(r_agent.epsilon, r_agent.epsilon_end)  # ç¡®ä¿ä¸ä½äºepsilon_end
            
            # æ‰“å°ç»Ÿè®¡ï¼ˆæ¯10ä¸ªepisodeï¼‰
            if episode % 10 == 0:
                avg_loss = sum(training_stats['losses'][-10:]) / min(10, len(training_stats['losses'])) if training_stats['losses'] else 0
                total_games = training_stats['wins'] + training_stats['losses_count'] + training_stats['draws']
                win_rate = training_stats['wins'] / max(1, total_games) * 100
                win_display = f"{training_stats['wins']}W/{training_stats['losses_count']}L/{training_stats['draws']}D (ç´¯ç§¯)"
                
                print(f"\nEpisode {episode} (å¹¶è¡Œ)")
                print(f"  å¹³å‡æŸå¤±: {avg_loss:.4f} | Lé˜ŸEpsilon: {l_agent.epsilon:.4f} | Ré˜ŸEpsilon: {r_agent.epsilon:.4f}")
                print(f"  Ré˜Ÿç­–ç•¥: {current_r_strategy} | ç´¯ç§¯èƒœç‡: {win_rate:.1f}% ({win_display}) [ç›®æ ‡: {TARGET_WIN_RATE}%] | è¿›åº¦: {episode}/{MAX_EPISODES}")
            
            # æ¨¡å‹è¯„ä¼°ï¼ˆæ¯1000ä¸ªepisodeï¼‰
            if episode % 1000 == 0 and episode > 0:
                avg_loss = sum(training_stats['losses'][-100:]) / min(100, len(training_stats['losses'])) if training_stats['losses'] else 0
                
                # è¿›è¡Œæ¨¡å‹è¯„ä¼°ï¼šè¿è¡Œ10æ¬¡æ¯”èµ›ä¸æœ€ä½³æ¨¡å‹å¯¹æ¯”
                print(f"\n{'='*60}")
                print(f"Episode {episode} - æ¨¡å‹è¯„ä¼°ä¸­...")
                print(f"{'='*60}")
                
                eval_win_rate, eval_wins, eval_losses, eval_draws = evaluate_model(l_agent, best_agent, num_games=10)
                
                print(f"  è¯„ä¼°ç»“æœ: {eval_win_rate:.1f}% ({eval_wins}W/{eval_losses}L/{eval_draws}D) vs æœ€ä½³æ¨¡å‹")
                print(f"  å†å²æœ€ä½³èƒœç‡: {best_win_rate:.1f}%")
                
                # å¦‚æœå½“å‰æ¨¡å‹æ›´å¥½ï¼Œæ›´æ–°æœ€ä½³æ¨¡å‹
                if eval_win_rate > best_win_rate:
                    best_win_rate = eval_win_rate
                    best_agent.q_network.load_state_dict(l_agent.q_network.state_dict())
                    best_agent.target_network.load_state_dict(l_agent.target_network.state_dict())
                    os.makedirs("models", exist_ok=True)
                    best_agent.save_model(best_model_path)
                    print(f"  ğŸ‰ æ–°æœ€ä½³æ¨¡å‹ï¼èƒœç‡: {best_win_rate:.1f}% (å·²ä¿å­˜åˆ° {best_model_path})")
                else:
                    print(f"  â„¹ï¸  å½“å‰æ¨¡å‹æœªè¶…è¶Šå†å²æœ€ä½³")
                
                print(f"{'='*60}")
                
                # ä½¿ç”¨è¯„ä¼°èƒœç‡è¿›è¡Œåˆ¤æ–­
                win_rate = eval_win_rate
                win_display = f"{eval_wins}W/{eval_losses}L/{eval_draws}D (è¯„ä¼°10å±€)"
                
                print(f"\nEpisode {episode} (è¯„ä¼°ç»“æœ)")
                print(f"  å¹³å‡æŸå¤±: {avg_loss:.4f} | Lé˜ŸEpsilon: {l_agent.epsilon:.4f} | Ré˜ŸEpsilon: {r_agent.epsilon:.4f}")
                print(f"  è¯„ä¼°èƒœç‡: {win_rate:.1f}% ({win_display}) [ç›®æ ‡: {TARGET_WIN_RATE}%] | è¿›åº¦: {episode}/{MAX_EPISODES}")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è®­ç»ƒæ¬¡æ•°
                if episode >= MAX_EPISODES:
                    print("\n" + "=" * 60)
                    print("ğŸ“Š è¾¾åˆ°æœ€å¤§è®­ç»ƒæ¬¡æ•°ï¼")
                    print("=" * 60)
                    print(f"å½“å‰Episode: {episode} / {MAX_EPISODES}")
                    print(f"å½“å‰èƒœç‡: {win_rate:.1f}% (ç›®æ ‡: {TARGET_WIN_RATE}%)")
                    print(f"æ€»æ¸¸æˆæ•°: {total_games}")
                    print("=" * 60)
                    print("\næ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹å¹¶åœæ­¢è®­ç»ƒ...")
                    
                    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
                    os.makedirs("models", exist_ok=True)
                    final_path = f"lib/models/dqn_model_final_ep{episode}.pth"
                    l_agent.save_model(final_path)
                    latest_path = "lib/models/dqn_model_latest.pth"
                    l_agent.save_model(latest_path)
                    
                    stats_path = "lib/models/training_stats.json"
                    with open(stats_path, 'w') as f:
                        json.dump(training_stats, f, indent=2)
                    
                    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
                    print(f"âœ… è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_path}")
                    
                    # åœæ­¢æ‰€æœ‰worker
                    print("\næ­£åœ¨åœæ­¢workerè¿›ç¨‹...")
                    for p in workers:
                        p.terminate()
                        p.join()
                    
                    if win_rate >= TARGET_WIN_RATE:
                        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²è¾¾åˆ°{TARGET_WIN_RATE}%èƒœç‡ç›®æ ‡ã€‚")
                    else:
                        print(f"\nè®­ç»ƒå®Œæˆï¼å½“å‰èƒœç‡ {win_rate:.1f}%ï¼Œæœªè¾¾åˆ° {TARGET_WIN_RATE}% ç›®æ ‡ã€‚")
                    break
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡èƒœç‡ï¼ˆ80%ï¼‰
                # ä½¿ç”¨è¯„ä¼°èƒœç‡è¿›è¡Œåˆ¤æ–­ï¼ˆä»…åœ¨è¯„ä¼°æ—¶æ£€æŸ¥ï¼‰
                if eval_win_rate >= TARGET_WIN_RATE:
                    print("\n" + "=" * 60)
                    print("ğŸ‰ è®­ç»ƒç›®æ ‡è¾¾æˆï¼")
                    print("=" * 60)
                    print(f"èƒœç‡: {win_rate:.1f}% >= 80.0%")
                    print(f"æ€»æ¸¸æˆæ•°: {total_games}")
                    print(f"å½“å‰Episode: {episode}")
                    print("=" * 60)
                    print("\næ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹å¹¶åœæ­¢è®­ç»ƒ...")
                    
                    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
                    os.makedirs("models", exist_ok=True)
                    final_path = "lib/models/dqn_model_final_winrate80.pth"
                    l_agent.save_model(final_path)
                    latest_path = "lib/models/dqn_model_latest.pth"
                    l_agent.save_model(latest_path)
                    
                    stats_path = "lib/models/training_stats.json"
                    with open(stats_path, 'w') as f:
                        json.dump(training_stats, f, indent=2)
                    
                    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
                    print(f"âœ… è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_path}")
                    
                    # åœæ­¢æ‰€æœ‰worker
                    print("\næ­£åœ¨åœæ­¢workerè¿›ç¨‹...")
                    for p in workers:
                        p.terminate()
                        p.join()
                    
                    print("\nè®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²è¾¾åˆ°80%èƒœç‡ç›®æ ‡ã€‚")
                    break
            
            # ä¿å­˜æ¨¡å‹ï¼ˆæ¯1000ä¸ªepisodeï¼Œé•¿æœŸè®­ç»ƒï¼‰
            if episode % 1000 == 0 and episode > 0:
                os.makedirs("models", exist_ok=True)
                model_path = f"lib/models/dqn_model_ep{episode}.pth"
                l_agent.save_model(model_path)
                latest_path = "lib/models/dqn_model_latest.pth"
                l_agent.save_model(latest_path)
                # ä»0è®­ç»ƒæ—¶ï¼ŒRé˜Ÿä¸åŠ è½½æ¨¡å‹ï¼Œåªä¿æŒé«˜æ¢ç´¢ç‡
                # ä»å·²æœ‰æ¨¡å‹ç»§ç»­æ—¶ï¼ŒRé˜ŸåŠ è½½æœ€æ–°æ¨¡å‹
                if LOAD_EXISTING_MODEL:
                    r_agent.load_model(latest_path)
                r_agent.epsilon = max(r_agent.epsilon, 0.15)  # ç¡®ä¿Ré˜Ÿä¿æŒé«˜æ¢ç´¢
                
                stats_path = "lib/models/training_stats.json"
                with open(stats_path, 'w') as f:
                    json.dump(training_stats, f, indent=2)
                
                # è®¡ç®—çª—å£èƒœç‡
                if len(evaluation_window) >= EVALUATION_WINDOW_SIZE:
                    window_wins = evaluation_window.count("WIN")
                    window_win_rate = window_wins / EVALUATION_WINDOW_SIZE * 100
                    current_win_rate = window_win_rate
                    win_rate_info = f"çª—å£èƒœç‡: {current_win_rate:.1f}% (æœ€è¿‘10å±€)"
                else:
                    total_games = training_stats['wins'] + training_stats['losses_count'] + training_stats['draws']
                    current_win_rate = training_stats['wins'] / max(1, total_games) * 100
                    win_rate_info = f"ç´¯ç§¯èƒœç‡: {current_win_rate:.1f}%"
                
                print(f"  âœ… æ¨¡å‹å·²ä¿å­˜: {latest_path}")
                print(f"  ğŸ“Š {win_rate_info} (ç›®æ ‡: {TARGET_WIN_RATE}%) | è¿›åº¦: {episode}/{MAX_EPISODES}")
            
            # å¯¹æ‰‹æ± ï¼šæ¯500ä¸ªepisodeä¿å­˜ä¸€æ¬¡å†å²æ¨¡å‹ï¼ˆä»…å½“ä»å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
            if LOAD_EXISTING_MODEL and episode % 500 == 0 and episode > 0:
                opponent_model_path = f"{opponent_pool_dir}/opponent_ep{episode}.pth"
                l_agent.save_model(opponent_model_path)
                opponent_pool.append(opponent_model_path)
                print(f"  âœ… å¯¹æ‰‹æ± æ–°å¢æ¨¡å‹: {opponent_model_path} (æ± å¤§å°: {len(opponent_pool)})")
                
                # é™åˆ¶å¯¹æ‰‹æ± å¤§å°ï¼ˆä¿ç•™æœ€è¿‘15ä¸ªæ¨¡å‹ï¼Œå¢åŠ å¤šæ ·æ€§ï¼‰
                if len(opponent_pool) > 15:
                    old_model = opponent_pool.pop(0)
                    try:
                        os.remove(old_model)
                        print(f"  ğŸ—‘ï¸  ç§»é™¤æ—§å¯¹æ‰‹æ¨¡å‹: {old_model}")
                    except:
                        pass
    
    except KeyboardInterrupt:
        print("\n\nè®­ç»ƒä¸­æ–­ï¼Œæ­£åœ¨åœæ­¢worker...")
        
        # åœæ­¢æ‰€æœ‰worker
        for p in workers:
            p.terminate()
            p.join()
        
        # è®¡ç®—æœ€ç»ˆèƒœç‡ï¼ˆä¼˜å…ˆä½¿ç”¨çª—å£èƒœç‡ï¼‰
        if len(evaluation_window) >= EVALUATION_WINDOW_SIZE:
            window_wins = evaluation_window.count("WIN")
            final_win_rate = window_wins / EVALUATION_WINDOW_SIZE * 100
            win_rate_type = "çª—å£èƒœç‡ï¼ˆæœ€è¿‘10å±€ï¼‰"
        else:
            total_games = training_stats['wins'] + training_stats['losses_count'] + training_stats['draws']
            final_win_rate = training_stats['wins'] / max(1, total_games) * 100
            win_rate_type = "ç´¯ç§¯èƒœç‡"
        
        total_games = training_stats['wins'] + training_stats['losses_count'] + training_stats['draws']
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        os.makedirs("models", exist_ok=True)
        final_path = "lib/models/dqn_model_final.pth"
        l_agent.save_model(final_path)
        latest_path = "lib/models/dqn_model_latest.pth"
        l_agent.save_model(latest_path)
        
        stats_path = "lib/models/training_stats.json"
        with open(stats_path, 'w') as f:
            json.dump(training_stats, f, indent=2)
        
        print(f"\næœ€ç»ˆç»Ÿè®¡:")
        print(f"  æ€»Episode: {training_stats['episode']} / {MAX_EPISODES}")
        print(f"  æ€»æ¸¸æˆæ•°: {total_games}")
        print(f"  {win_rate_type}: {final_win_rate:.1f}% (ç›®æ ‡: {TARGET_WIN_RATE}%)")
        if len(evaluation_window) >= EVALUATION_WINDOW_SIZE:
            window_wins = evaluation_window.count("WIN")
            window_losses = evaluation_window.count("LOSS")
            window_draws = evaluation_window.count("DRAW")
            print(f"  æœ€è¿‘10å±€: {window_wins}W/{window_losses}L/{window_draws}D")
        print(f"  ç´¯ç§¯è®°å½•: {training_stats['wins']}W/{training_stats['losses_count']}L/{training_stats['draws']}D")
        print(f"\nâœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
        print(f"âœ… è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_path}")
        
        if final_win_rate >= TARGET_WIN_RATE:
            print(f"\nğŸ‰ æ­å–œï¼æ¨¡å‹å·²è¾¾åˆ°{TARGET_WIN_RATE}%èƒœç‡ç›®æ ‡ï¼")
        else:
            print(f"\nğŸ’¡ å½“å‰èƒœç‡ {final_win_rate:.1f}%ï¼Œç»§ç»­è®­ç»ƒå¯è¾¾åˆ°{TARGET_WIN_RATE}%ç›®æ ‡")


if __name__ == "__main__":
    # å¤šè¿›ç¨‹éœ€è¦è¿™ä¸ªä¿æŠ¤
    multiprocessing.set_start_method('spawn', force=True)
    main()

